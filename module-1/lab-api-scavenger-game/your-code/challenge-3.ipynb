{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moved to Jupyter notebook for Challenge 3, just expecting to keep hitting the rate_limit otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPENDENCIES\n",
    "from keyring import github_token\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# RESOURCES\n",
    "# https://developer.github.com/v3/search/#search-repositories\n",
    "# up to 1,000 results per search\n",
    "# https://developer.github.com/v3/search/#constructing-a-search-query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resources': {'core': {'limit': 60, 'remaining': 0, 'reset': 1569801751}, 'search': {'limit': 10, 'remaining': 10, 'reset': 1569798685}, 'graphql': {'limit': 0, 'remaining': 0, 'reset': 1569802225}, 'integration_manifest': {'limit': 5000, 'remaining': 5000, 'reset': 1569802172}}, 'rate': {'limit': 60, 'remaining': 0, 'reset': 1569801751}}\n"
     ]
    }
   ],
   "source": [
    "rate_message = (requests.get('https://api.github.com/rate_limit')).json()\n",
    "print(rate_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     .gitignore\n",
      "1          15024\n",
      "2          15534\n",
      "3          17020\n",
      "4          30351\n",
      "5          40303\n",
      "6          44639\n",
      "7          45525\n",
      "8          47222\n",
      "9          47830\n",
      "10         49418\n",
      "11         50896\n",
      "12         55417\n",
      "13         55685\n",
      "14         60224\n",
      "15         64880\n",
      "16         66032\n",
      "17         68848\n",
      "18         70751\n",
      "19         70985\n",
      "20         88596\n",
      "21         89046\n",
      "22         89338\n",
      "23         91701\n",
      "24         97881\n",
      "25         98750\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check to make sure I've got the right search query\n",
    "# url = 'https://api.github.com/search/repositories?q=stars:>=10000'\n",
    "# url = 'https://api.github.com/search/code?q=filename:scavenger+repo:ironhack-labs/data-labs'\n",
    "# url = 'https://api.github.com/search/code?q=filename:scavenger+org:ironhack-labs'\n",
    "# q=repo:ironhack-labs/data-labs/+filename:.scavengerhunt # not found\n",
    "# q=filename:.scavengerhunt+repo:ironhack-labs/data-labs/ # not found\n",
    "# q=repo:ironhack-labs/data-labs/+filename:scavengerhunt # not found\n",
    "# q=filename:scavengerhunt+in:repo:ironhack-labs/data-labs # not found\n",
    "# q=filename:scavengerhunt+repo:ironhack-labs/data-labs # not found\n",
    "# q=org:Ironhack-labs+repo:ironhack-labs/data-labs # not found\n",
    "# from terminal, attempted\n",
    "# curl -u SamPoad:\"my token string here\" \"https://api.github.com/Search/repositories/q=repo:ironhack-labs/data-labs\"\n",
    "# looks like the below is the better format to use?\n",
    "# https://api.github.com/Search/repositories?q=repo:ironhack-labs/data-labs\n",
    "# and still doesn't work\n",
    "# at least I know it CAN be done\n",
    "# https://api.github.com/search/repositories?q=stars:>=10000+language:go&sort=stars&order=desc\n",
    "# url = 'https://api.github.com/Search/repositories?q=scavengerhunt'\n",
    "# not even the above works\n",
    "# FINALLY found this file\n",
    "# https://api.github.com/search/code?q=scavenger+repo:ironhack-labs/data-labs\n",
    "# url = 'https://api.github.com/search/code?q=scavenger+repo:ironhack-labs/data-labs\n",
    "# https://developer.github.com/v3/repos/contents/#get-contents\n",
    "# https://api.github.com/search/repositories?q=repo:ironhack-labs/data-labs\n",
    "# https://github.com/ironhack-datalabs/scavenger\n",
    "# ^^^ This is the repo I should be looking in per the ReadMe, not ironhack-labs/data-labs.\n",
    "# GET /repos/:owner/:repo/contents/:path\n",
    "# https://api.github.com/repos/ironhack-datalabs/scavenger/contents/\n",
    "# ^^^ this seems to work. need to iterate through the subfolders?\n",
    "# https://api.github.com/repos/ironhack-datalabs/scavenger/contents/15024 is a subfolder,\n",
    "#     so for folder in url/contents/:\n",
    "#         if the filename contains 'scavenger':\n",
    "#             add it to a list\n",
    "# do the above with a nested for loop? \n",
    "\n",
    "# Ran the following in terminal so I could see the number of pages\n",
    "# curl -I \"https://api.github.com/repos/ironhack-datalabs/scavenger/contents\"\n",
    "# this returned without any pagination, which makes sense. the default response is 30 entries, and there's only 25 folders\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nested For Loop, actually doesn't need to be nested it I just pull out the value from key:value of name:foldername\n",
    "contents_url = 'https://api.github.com/repos/ironhack-datalabs/scavenger/contents'\n",
    "foldernames = []\n",
    "response = requests.get(contents_url)\n",
    "result = response.json()\n",
    "folder_dict = pd.DataFrame(result)\n",
    "print(folder_dict['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              _links  \\\n",
      "0  {'self': 'https://api.github.com/repos/ironhac...   \n",
      "1  {'self': 'https://api.github.com/repos/ironhac...   \n",
      "2  {'self': 'https://api.github.com/repos/ironhac...   \n",
      "3  {'self': 'https://api.github.com/repos/ironhac...   \n",
      "4  {'self': 'https://api.github.com/repos/ironhac...   \n",
      "\n",
      "                                        download_url  \\\n",
      "0  https://raw.githubusercontent.com/ironhack-dat...   \n",
      "1                                               None   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "\n",
      "                                             git_url  \\\n",
      "0  https://api.github.com/repos/ironhack-datalabs...   \n",
      "1  https://api.github.com/repos/ironhack-datalabs...   \n",
      "2  https://api.github.com/repos/ironhack-datalabs...   \n",
      "3  https://api.github.com/repos/ironhack-datalabs...   \n",
      "4  https://api.github.com/repos/ironhack-datalabs...   \n",
      "\n",
      "                                            html_url        name        path  \\\n",
      "0  https://github.com/ironhack-datalabs/scavenger...  .gitignore  .gitignore   \n",
      "1  https://github.com/ironhack-datalabs/scavenger...       15024       15024   \n",
      "2  https://github.com/ironhack-datalabs/scavenger...       15534       15534   \n",
      "3  https://github.com/ironhack-datalabs/scavenger...       17020       17020   \n",
      "4  https://github.com/ironhack-datalabs/scavenger...       30351       30351   \n",
      "\n",
      "                                        sha  size  type  \\\n",
      "0  e43b0f988953ae3a84b00331d0ccf5f7d51cb3cf    10  file   \n",
      "1  2945e51c87ad5da893c954afcf092f06343bbb7d     0   dir   \n",
      "2  5af6f2a7287e4191f39e55693fc1e9c8918d1d3a     0   dir   \n",
      "3  9c49f920aa4d9433fa99a5824128f0e6b90ec5f2     0   dir   \n",
      "4  c488d7f64088c852e22067d48fdc64ee3670f3ba     0   dir   \n",
      "\n",
      "                                                 url  \n",
      "0  https://api.github.com/repos/ironhack-datalabs...  \n",
      "1  https://api.github.com/repos/ironhack-datalabs...  \n",
      "2  https://api.github.com/repos/ironhack-datalabs...  \n",
      "3  https://api.github.com/repos/ironhack-datalabs...  \n",
      "4  https://api.github.com/repos/ironhack-datalabs...  \n"
     ]
    }
   ],
   "source": [
    "for i in folder_dict['name']: # for each folder in the scavenger repo\n",
    "    sub_url = f'https://api.github.com/repos/ironhack-datalabs/scavenger/contents/{i}' # use the api to open that folder\n",
    "    for file in sub_url: #for each file within the sub-url (NOPE determined this is reading as \"for each character in the URL string\")\n",
    "        sub_url2 = f'https://api.github.com/repos/ironhack-datalabs/scavenger/contents/{i}/{file}'\n",
    "        response2 = requests.get(sub_url2) # if that filename has \"scavenger\" in it\n",
    "        result2 = response2.json()   \n",
    "        scav_files_df = pd.DataFrame(result) # throw it in a dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_links          object\n",
      "download_url    object\n",
      "git_url         object\n",
      "html_url        object\n",
      "name            object\n",
      "path            object\n",
      "sha             object\n",
      "size             int64\n",
      "type            object\n",
      "url             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(scav_files_df.head())\n",
    "print(scav_files_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see from above that there is a download_url in the dataframe.\n",
    "probably my best bet to reading the files themselves.\n",
    "look up previous labwork that covered reading text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/ironhack-datalabs/scavenger/master/.gitignore\n"
     ]
    }
   ],
   "source": [
    "print(scav_files_df['download_url'][0]) # or this is just the .gitignore file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': 'https://api.github.com/repos/ironhack-datalabs/scavenger/contents/15024?ref=master', 'git': 'https://api.github.com/repos/ironhack-datalabs/scavenger/git/trees/2945e51c87ad5da893c954afcf092f06343bbb7d', 'html': 'https://github.com/ironhack-datalabs/scavenger/tree/master/15024'}\n"
     ]
    }
   ],
   "source": [
    "print(scav_files_df['_links'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                documentation_url  \\\n",
      "0  https://developer.github.com/v3/#rate-limiting   \n",
      "\n",
      "                                             message  \n",
      "0  API rate limit exceeded for 73.138.93.245. (Bu...  \n"
     ]
    }
   ],
   "source": [
    "for i in folder_dict['name']: # for each folder in the scavenger repo\n",
    "    sub_url = f'https://api.github.com/repos/ironhack-datalabs/scavenger/contents/{i}' # use the api to open that folder\n",
    "    for file in requests.get(sub_url): #for each file within the sub-url\n",
    "        sub_url2 = f'https://api.github.com/repos/ironhack-datalabs/scavenger/contents/{i}/{file}'\n",
    "#         print(file[\"name\"]) returns as a byte\n",
    "        response2 = requests.get(sub_url2) # if that filename has \"scavenger\" in it\n",
    "        result2 = response2.json()\n",
    "        flat_result2 = json_normalize(result2)\n",
    "#         scav_files_df = pd.DataFrame(result2) # throw it in a dataframe\n",
    "print(flat_result2)\n",
    "# this method results in the API rate limit getting hit. need to refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     .gitignore\n",
      "1          15024\n",
      "2          15534\n",
      "3          17020\n",
      "4          30351\n",
      "5          40303\n",
      "6          44639\n",
      "7          45525\n",
      "8          47222\n",
      "9          47830\n",
      "10         49418\n",
      "11         50896\n",
      "12         55417\n",
      "13         55685\n",
      "14         60224\n",
      "15         64880\n",
      "16         66032\n",
      "17         68848\n",
      "18         70751\n",
      "19         70985\n",
      "20         88596\n",
      "21         89046\n",
      "22         89338\n",
      "23         91701\n",
      "24         97881\n",
      "25         98750\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(scav_files_df.dtypes)\n",
    "# print(scav_files_df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapped this\n",
    "# to try and pull in all the files? \n",
    "for i in range(.0001, .0024):\n",
    "    url = f'https://api.github.com/Search/repositories/q=repo:ironhack-labs/data-labs/filename:{i}.scavengerhunt'\n",
    "    response = requests.get(url)\n",
    "    result = pd.read_json(response.json())\n",
    "    result2.append(result)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far, I have that I need to loop through \n",
    "the raw.github url like this one\n",
    "https://raw.githubusercontent.com/ironhack-datalabs/scavenger/master/15024/.0006.scavengerhunt\n",
    "so that I can have the system READ and store the data of the scavengerhunt files.\n",
    "\n",
    "i'm stuck on getting to that point, still held up on using the API to get exactly what I want as a dictionary.\n",
    "latest attempt is to flatten it for easier retrieval of the raw.github URLs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
