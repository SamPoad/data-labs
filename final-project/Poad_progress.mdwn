# Progress Log

Project began on Feb 25, 2020

Decided to use a Malaria Dataset
<br> taken from https://lhncbc.nlm.nih.gov/publication/pub9932

the plan is to use this dataset to build a predictive model on whether a new entry has malaria or not.

Domain knowledge a bit to go into the staining process to identify malaria in cells, and then how to address visually.

Interesting thing to note about the data
* The CSV file for the uninfected class contains 201 entries since the normal cells from the infected patientsâ€™ slides also make it to the normal cell category (151+50 = 201)

Remember that this will all be a Python file eventually, so pickle the model that you settle on. 

plan - start a Jupyter notebook for relevant sections

Day 1 target - read the images as arrays, take this data and store as a SQL database.
(this will show Python knowledge)
one table as infected, another table as uninfected

Read in the SQL data to start the model (show SQL knowledge)
concat the dataframes and create a target column to identify Malaria (1) or not (0)

the Features of the dataset will be the array information itself.

Review the image processing / neural network lesson - do i need to reduce the complexity of the images at all? 
My images are very small, and few colors, but I have 27.5k of them to process.

Get an overall sense of SIZE of the data. roughly 28,000 images at what pixel height/width? 

Day 1 - ran into - how to get these images into their array of pixel values? 
research: 
https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/

combined this with existing knowledge of matplotlib.pyplot and matplotlib.Image

first obstacle - how to read in images as an array of pixel data? - solved
second obstacle - how to create a dataframe out of this? because this dataframe will need to be written to a database. while working on this, ran into...
third obstacle - the images are not all the same dimensions, so I'll need to fill in those with null values (can my model handle
null values? maybe continue with the [0, 0, 0,] for black that the images already use?)
    so for both of the image_quantization and the neural_network exercises, we read in those arrays and flatten/ravel them 
    so that the end result of the array is a string of rgbrgbrgb
    similar to the neural network array used.
    (spoke with instructor during this portion, recommended to treat the SQL save and load as a bonus)
    worked a bit more on trying to read/append the arrays into a dataframe, decided to treat the collection as an array, and work 
    out the train_test split of "features" and "target" using array math instead?

Day 2 target - read the SQL together, and concat the dataframes to create the dataset I need to be using. 
Next - work some heavy Principal Component Analysis (PCA) from Module-2 for reducing dimensionality of the images? 
or reduce it to black and white based on whether the stain appears or not?
Day 2 Actual - read in all the images and save to a SQL database
images come in like 
array([[[0., 0., 0.], this is image1, row1, pixel1
        [0., 0., 0.], this is image1, row1, pixel2
        [0., 0., 0.], this is image1, row1, pixel3
        ...,
so i've read each of the parasitized image arrays into a dataframe. remember to git commit this.
but I need to instead build the dataframe so that it has columns for each pixel? so the 3 values for the pixel would be 
a list of 3 numbers within that one cell?
end of day 2 - learned more about my image data, steps to process

### Overall plan
read in and review the image data - exploratory

read in the parasitized images
convert to an array
flatten the array, convert the RGB values to grayscale
normalize/scale the data between 0 and 1 (is that what grayscale will be doing?)
keras.normalize between 0 and 1
throw this into a dataframe
label as parasitized
save that to a database table (optional bonus)

read in the normal images
convert to an array
flatten the array, convert the RGB values to grayscale
normalize/scale the data between 0 and 1 (is that what grayscale will be doing?)
keras.normalize between 0 and 1
label as non-parasitized
save that to a database table (optional bonus)

read in the two tables (optional bonus)
concat/join/merge the dataframes
confirm the ~50% split between Classes

train_test_split the data

initialize a Sequential Model

add input layers

compile

determine accuracy

tweak the hyperparameters, determine accuracy at each step

save the final model (pickle it)

grab a few images and run .predict, using .imshow() and .imsave() to grab the images for presentation

create a python file to import the model as a demo


